{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "\n",
    "import arrow\n",
    "import jmespath\n",
    "from scrapfly import ScrapeApiResponse, ScrapeConfig, ScrapflyClient\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARINE = \"https://www.redfin.com/county/323/CA/Marin-County\"\n",
    "API_KEY = \"scp-live-xxx\"  # API key to be filled here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapfly = ScrapflyClient(key=API_KEY, max_concurrency=2)\n",
    "\n",
    "def extract_cache(react_initial_context):\n",
    "    \"\"\"extract microservice cache data from the react server agent\"\"\"\n",
    "    result = {}\n",
    "    for name, cache in react_initial_context[\"ReactServerAgent.cache\"][\"dataCache\"].items():\n",
    "        # first we retrieve cached response and see whether it's a success\n",
    "        try:\n",
    "            cache_response = cache[\"res\"]\n",
    "        except KeyError:  # empty cache\n",
    "            continue\n",
    "        if cache_response.get(\"status\") != 200:\n",
    "            print(\"skipping non 200 cache\")\n",
    "            continue\n",
    "        # then extract cached response body and interpret it as a JSON\n",
    "        cache_data = cache_response.get(\"body\", {}).get(\"payload\")\n",
    "        if not cache_data:\n",
    "            cache_data = json.loads(cache_response[\"text\"].split(\"&&\", 1)[-1]).get(\"payload\")\n",
    "        if not cache_data:\n",
    "            # skip empty caches\n",
    "            continue\n",
    "        # for Redfin we can cleanup cache names for home data endpoints:\n",
    "        if \"/home/details\" in name:\n",
    "            name = name.split(\"/home/details/\")[-1]\n",
    "        result[name.replace(\"/\", \"\")] = cache_data\n",
    "        # ^note: we sanitize name to avoid slashes as they are not allowed in JMESPath\n",
    "    return result\n",
    "\n",
    "\n",
    "class PropertyResult(TypedDict):\n",
    "    \"\"\"type hint for property result. i.e. Defines what fields are expected in property dataset\"\"\"\n",
    "\n",
    "    photos: List[str]\n",
    "    videos: List[str]\n",
    "    price: int\n",
    "    info: Dict[str, str]\n",
    "    amenities: List[Dict[str, str]]\n",
    "    records: Dict[str, str]\n",
    "    history: Dict[str, str]\n",
    "    floorplan: Dict[str, str]\n",
    "    activity: Dict[str, str]\n",
    "\n",
    "\n",
    "def parse_redfin_proprety_cache(data_cache) -> PropertyResult:\n",
    "    \"\"\"parse Redfin's cache data for proprety information\"\"\"\n",
    "    # here we define field name to JMESPath mapping\n",
    "    parse_map = {\n",
    "        # from top area of the page: basic info, videos and photos\n",
    "        \"photos\": \"aboveTheFold.mediaBrowserInfo.photos[*].photoUrls.fullScreenPhotoUrl\",\n",
    "        \"videos\": \"aboveTheFold.mediaBrowserInfo.videos[*].videoUrl\",\n",
    "        \"price\": \"aboveTheFold.addressSectionInfo.priceInfo.amount\",\n",
    "        \"info\": \"\"\"aboveTheFold.addressSectionInfo.{\n",
    "            bed_num: beds,\n",
    "            bath_numr: baths,\n",
    "            full_baths_num: numFullBaths,\n",
    "            sqFt: sqFt,\n",
    "            year_built: yearBuitlt,\n",
    "            city: city,\n",
    "            state: state,\n",
    "            zip: zip,\n",
    "            country_code: countryCode,\n",
    "            fips: fips,\n",
    "            apn: apn,\n",
    "            redfin_age: timeOnRedfin,\n",
    "            cumulative_days_on_market: cumulativeDaysOnMarket,\n",
    "            property_type: propertyType,\n",
    "            listing_type: listingType,\n",
    "            url: url\n",
    "        }\n",
    "        \"\"\",\n",
    "        # from bottom area of the page: amenities, records and event history\n",
    "        \"amenities\": \"\"\"belowTheFold.amenitiesInfo.superGroups[].amenityGroups[].amenityEntries[].{\n",
    "            name: amenityName, values: amenityValues\n",
    "        }\"\"\",\n",
    "        \"records\": \"belowTheFold.publicRecordsInfo\",\n",
    "        \"history\": \"belowTheFold.propertyHistoryInfo\",\n",
    "        # other: sometimes there are floorplans\n",
    "        \"floorplan\": r\"listingfloorplans.floorPlans\",\n",
    "        # and there's always internal Redfin performance info: views, saves, etc.\n",
    "        \"activity\": \"activityInfo\",\n",
    "    }\n",
    "    results = {}\n",
    "    for key, path in parse_map.items():\n",
    "        value = jmespath.search(path, data_cache)\n",
    "        results[key] = value\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_property(result: ScrapeApiResponse) -> PropertyResult:\n",
    "    script = result.selector.xpath('//script[contains(.,\"ServerState.InitialContext\")]/text()').get()\n",
    "    initial_context = re.findall(r\"ServerState.InitialContext = (\\{.+\\});\", script)\n",
    "    if not initial_context:\n",
    "        print(f\"page {result.context['url']} is not a property listing page\")\n",
    "        return\n",
    "    return parse_redfin_proprety_cache(extract_cache(json.loads(initial_context[0])))\n",
    "\n",
    "\n",
    "async def scrape_properties(urls: List[str]) -> List[PropertyResult]:\n",
    "    to_scrape = [ScrapeConfig(url=url, asp=True, country=\"US\", cache=True) for url in urls]\n",
    "    properties = []\n",
    "    async for result in scrapfly.concurrent_scrape(to_scrape):\n",
    "        properties.append(parse_property(result))\n",
    "    return properties\n",
    "\n",
    "\n",
    "\n",
    "async def scrape_feed(url) -> Dict[str, datetime]:\n",
    "    \"\"\"Scrape Redfin sitemap for URLs\"\"\"\n",
    "    result = await scrapfly.async_scrape(ScrapeConfig(url=url, country=\"US\", cache=True, asp=True))\n",
    "    results = {}\n",
    "    for item in result.selector.xpath(\"//url\"):\n",
    "        url = item.xpath(\"loc/text()\").get()\n",
    "        pub_date = item.xpath(\"lastmod/text()\").get()\n",
    "        results[url] = arrow.get(pub_date).datetime\n",
    "    return results\n",
    "\n",
    "async def get_data():\n",
    "    feed = await scrape_feed(\"https://www.redfin.com/county/345/CA/Santa-Clara-County/filter/mr=5:323+5:330\")\n",
    "    asyncio.run(scrape_feed(\"https://www.redfin.com/newest_listings.xml\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.run(get_data())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
